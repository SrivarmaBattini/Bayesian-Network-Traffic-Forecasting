{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ba0382a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sriva\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level keys inside 'pems-bay.h5': ['speed']\n",
      "Subkeys inside 'speed': ['axis0', 'axis1', 'block0_items', 'block0_values']\n",
      "✅ Loaded 'speed/block0_values' successfully.\n",
      "PEMS-BAY data shape: (52116, 325)\n",
      "Adjacency matrix shape: (325, 325)\n",
      "Metadata keys: ['meta']\n",
      "\n",
      "✅ Dataset, adjacency matrix, and metadata loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================\n",
    "# PART 1: Load PEMS-BAY Dataset\n",
    "# ==============================\n",
    "\n",
    "# Step 1: Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle  \n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "# Step 2: Load main dataset (pems-bay.h5)\n",
    "with h5py.File(\"pems-bay.h5\", \"r\") as f:\n",
    "    print(\"Top-level keys inside 'pems-bay.h5':\", list(f.keys()))\n",
    "\n",
    "    # The real numeric data is stored under 'speed/block0_values'\n",
    "    if 'speed' in f.keys():\n",
    "        print(\"Subkeys inside 'speed':\", list(f['speed'].keys()))\n",
    "        data = np.array(f['speed']['block0_values'])\n",
    "        print(\"✅ Loaded 'speed/block0_values' successfully.\")\n",
    "    else:\n",
    "        raise ValueError(\"'speed' key not found in file\")\n",
    "\n",
    "print(\"PEMS-BAY data shape:\", data.shape)\n",
    "# Expected shape: (52116, 325) -> 52116 time steps × 325 sensors\n",
    "\n",
    "# Step 3: Load adjacency matrix (adj_mx_bay.pkl)\n",
    "with open(\"adj_mx_bay.pkl\", \"rb\") as f:\n",
    "    adj_mx = pickle.load(f, encoding=\"latin1\")\n",
    "    adjacency = adj_mx[2]  # third element is adjacency matrix\n",
    "print(\"Adjacency matrix shape:\", adjacency.shape)\n",
    "\n",
    "# Step 4: Load metadata (pems-bay-meta.h5)\n",
    "with h5py.File(\"pems-bay-meta.h5\", \"r\") as f:\n",
    "    meta_keys = list(f.keys())\n",
    "    meta = {key: np.array(f[key]) for key in f.keys()}\n",
    "print(\"Metadata keys:\", meta_keys)\n",
    "\n",
    "# Step 5: Confirm successful loading\n",
    "print(\"\\n✅ Dataset, adjacency matrix, and metadata loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e4480bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset data shape: (52116, 3)\n",
      "   Upstream  Current  Downstream\n",
      "0      67.4     68.9        69.5\n",
      "1      67.7     68.9        70.0\n",
      "2      67.0     68.8        69.6\n",
      "3      67.4     69.2        70.0\n",
      "4      67.0     69.0        69.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Select 3 consecutive sensors (in paper terms: upstream, current, downstream)\n",
    "sensor_ids = [100, 101, 102]  # column indices for sensors\n",
    "subset_data = data[:, sensor_ids]  # shape = (time_steps, 3)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(subset_data, columns=['Upstream', 'Current', 'Downstream'])\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"Subset data shape:\", df.shape)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a04455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with time-lag features added:\n",
      "   Upstream  Current  Downstream  Upstream_prev  Current_prev  Downstream_prev\n",
      "0      67.7     68.9        70.0           67.4          68.9             69.5\n",
      "1      67.0     68.8        69.6           67.7          68.9             70.0\n",
      "2      67.4     69.2        70.0           67.0          68.8             69.6\n",
      "3      67.0     69.0        69.3           67.4          69.2             70.0\n",
      "4      67.2     68.6        69.5           67.0          69.0             69.3\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Add previous-time-step columns (temporal dependency)\n",
    "df['Upstream_prev'] = df['Upstream'].shift(1)\n",
    "df['Current_prev'] = df['Current'].shift(1)\n",
    "df['Downstream_prev'] = df['Downstream'].shift(1)\n",
    "\n",
    "# Drop first row (NaN) created by shift\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "print(\"Data with time-lag features added:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "274f9b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretized data sample:\n",
      "   Upstream  Current  Downstream  Upstream_prev  Current_prev  Downstream_prev\n",
      "0         3        4           4              3             4                4\n",
      "1         3        4           4              3             4                4\n",
      "2         3        4           4              3             4                4\n",
      "3         3        4           4              3             4                4\n",
      "4         3        4           4              3             4                4\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Discretize continuous speed values into 5 categories (very low → very high)\n",
    "for col in df.columns:\n",
    "    df[col] = pd.cut(df[col], bins=5, labels=False)\n",
    "\n",
    "print(\"Discretized data sample:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "413186c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Network structure defined with edges:\n",
      "   ('Upstream_prev', 'Upstream')\n",
      "   ('Upstream', 'Current')\n",
      "   ('Current_prev', 'Current')\n",
      "   ('Current', 'Downstream')\n",
      "   ('Downstream_prev', 'Downstream')\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Define BN structure (spatio-temporal)\n",
    "from pgmpy.models import DiscreteBayesianNetwork  # updated import\n",
    "\n",
    "edges = [\n",
    "    ('Upstream_prev', 'Upstream'),\n",
    "    ('Upstream', 'Current'),\n",
    "    ('Current_prev', 'Current'),\n",
    "    ('Current', 'Downstream'),\n",
    "    ('Downstream_prev', 'Downstream')\n",
    "]\n",
    "\n",
    "model = DiscreteBayesianNetwork(edges)  # updated class name\n",
    "print(\"Bayesian Network structure defined with edges:\")\n",
    "for e in edges:\n",
    "    print(\"  \", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1fd0c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pgmpy: Datatype (N=numerical, C=Categorical Unordered, O=Categorical Ordered) inferred from data: \n",
      " {'Upstream': 'N', 'Current': 'N', 'Downstream': 'N', 'Upstream_prev': 'N', 'Current_prev': 'N', 'Downstream_prev': 'N'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian Network fitted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Fit model using MLE\n",
    "model.fit(df, estimator=MaximumLikelihoodEstimator)\n",
    "print(\"Bayesian Network fitted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f80c598f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Current | Upstream=3, Current_prev=2):\n",
      "+------------+----------------+\n",
      "| Current    |   phi(Current) |\n",
      "+============+================+\n",
      "| Current(0) |         0.0000 |\n",
      "+------------+----------------+\n",
      "| Current(1) |         0.0202 |\n",
      "+------------+----------------+\n",
      "| Current(2) |         0.8367 |\n",
      "+------------+----------------+\n",
      "| Current(3) |         0.1431 |\n",
      "+------------+----------------+\n",
      "| Current(4) |         0.0000 |\n",
      "+------------+----------------+ \n",
      "\n",
      "P(Downstream | Current=3):\n",
      "+---------------+-------------------+\n",
      "| Downstream    |   phi(Downstream) |\n",
      "+===============+===================+\n",
      "| Downstream(0) |            0.0009 |\n",
      "+---------------+-------------------+\n",
      "| Downstream(1) |            0.0030 |\n",
      "+---------------+-------------------+\n",
      "| Downstream(2) |            0.0444 |\n",
      "+---------------+-------------------+\n",
      "| Downstream(3) |            0.4237 |\n",
      "+---------------+-------------------+\n",
      "| Downstream(4) |            0.5280 |\n",
      "+---------------+-------------------+ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Perform inference using Variable Elimination\n",
    "infer = VariableElimination(model)\n",
    "\n",
    "# Predict traffic level of 'Current' given upstream and current_prev states\n",
    "q1 = infer.query(variables=['Current'], evidence={'Upstream': 3, 'Current_prev': 2})\n",
    "print(\"P(Current | Upstream=3, Current_prev=2):\")\n",
    "print(q1, \"\\n\")\n",
    "\n",
    "# Predict downstream given current\n",
    "q2 = infer.query(variables=['Downstream'], evidence={'Current': 3})\n",
    "print(\"P(Downstream | Current=3):\")\n",
    "print(q2, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35a9520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure and CPDs are consistent.\n",
      "\n",
      "Upstream_prev -> parents: []\n",
      "Upstream -> parents: ['Upstream_prev']\n",
      "Current -> parents: ['Upstream', 'Current_prev']\n",
      "Current_prev -> parents: []\n",
      "Downstream -> parents: ['Current', 'Downstream_prev']\n",
      "Downstream_prev -> parents: []\n",
      "\n",
      "Direct causes of 'Current': ['Upstream', 'Current_prev']\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Validate model\n",
    "if model.check_model():\n",
    "    print(\"Model structure and CPDs are consistent.\\n\")\n",
    "else:\n",
    "    print(\"Model invalid. Please check data.\")\n",
    "\n",
    "# Step 12: Display node parents (causal relationships)\n",
    "for node in model.nodes():\n",
    "    print(f\"{node} -> parents: {model.get_parents(node)}\")\n",
    "\n",
    "print(\"\\nDirect causes of 'Current':\", model.get_parents('Current'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5b38644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE between predicted and actual traffic states: 0.179\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Simple evaluation (optional)\n",
    "# Forecast Current based on evidence of previous step\n",
    "y_true = df['Current'].values[1:]\n",
    "y_pred = []\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    try:\n",
    "        q = infer.query(variables=['Current'],\n",
    "                        evidence={'Upstream': int(df.loc[i-1,'Upstream']),\n",
    "                                  'Current_prev': int(df.loc[i-1,'Current_prev'])})\n",
    "        pred = q.values.argmax()\n",
    "    except:\n",
    "        pred = df.loc[i-1,'Current_prev']  # fallback\n",
    "    y_pred.append(pred)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "rmse = math.sqrt(mean_squared_error(y_true[:len(y_pred)], y_pred))\n",
    "print(f\"RMSE between predicted and actual traffic states: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80963e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
